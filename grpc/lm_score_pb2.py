# Generated by the protocol buffer compiler.  DO NOT EDIT!
# source: lm_score.proto

import sys
_b=sys.version_info[0]<3 and (lambda x:x) or (lambda x:x.encode('latin1'))
from google.protobuf import descriptor as _descriptor
from google.protobuf import message as _message
from google.protobuf import reflection as _reflection
from google.protobuf import symbol_database as _symbol_database
from google.protobuf import descriptor_pb2
# @@protoc_insertion_point(imports)

_sym_db = _symbol_database.Default()




DESCRIPTOR = _descriptor.FileDescriptor(
  name='lm_score.proto',
  package='lm',
  syntax='proto3',
  serialized_pb=_b('\n\x0elm_score.proto\x12\x02lm\"#\n\x0fSentenceRequest\x12\x10\n\x08sentence\x18\x01 \x01(\t\" \n\x0fLMScoreResponse\x12\r\n\x05score\x18\x01 \x01(\x02\x32\x44\n\rLanguageModel\x12\x33\n\x05Score\x12\x13.lm.SentenceRequest\x1a\x13.lm.LMScoreResponse\"\x00\x62\x06proto3')
)
_sym_db.RegisterFileDescriptor(DESCRIPTOR)




_SENTENCEREQUEST = _descriptor.Descriptor(
  name='SentenceRequest',
  full_name='lm.SentenceRequest',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='sentence', full_name='lm.SentenceRequest.sentence', index=0,
      number=1, type=9, cpp_type=9, label=1,
      has_default_value=False, default_value=_b("").decode('utf-8'),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=22,
  serialized_end=57,
)


_LMSCORERESPONSE = _descriptor.Descriptor(
  name='LMScoreResponse',
  full_name='lm.LMScoreResponse',
  filename=None,
  file=DESCRIPTOR,
  containing_type=None,
  fields=[
    _descriptor.FieldDescriptor(
      name='score', full_name='lm.LMScoreResponse.score', index=0,
      number=1, type=2, cpp_type=6, label=1,
      has_default_value=False, default_value=float(0),
      message_type=None, enum_type=None, containing_type=None,
      is_extension=False, extension_scope=None,
      options=None),
  ],
  extensions=[
  ],
  nested_types=[],
  enum_types=[
  ],
  options=None,
  is_extendable=False,
  syntax='proto3',
  extension_ranges=[],
  oneofs=[
  ],
  serialized_start=59,
  serialized_end=91,
)

DESCRIPTOR.message_types_by_name['SentenceRequest'] = _SENTENCEREQUEST
DESCRIPTOR.message_types_by_name['LMScoreResponse'] = _LMSCORERESPONSE

SentenceRequest = _reflection.GeneratedProtocolMessageType('SentenceRequest', (_message.Message,), dict(
  DESCRIPTOR = _SENTENCEREQUEST,
  __module__ = 'lm_score_pb2'
  # @@protoc_insertion_point(class_scope:lm.SentenceRequest)
  ))
_sym_db.RegisterMessage(SentenceRequest)

LMScoreResponse = _reflection.GeneratedProtocolMessageType('LMScoreResponse', (_message.Message,), dict(
  DESCRIPTOR = _LMSCORERESPONSE,
  __module__ = 'lm_score_pb2'
  # @@protoc_insertion_point(class_scope:lm.LMScoreResponse)
  ))
_sym_db.RegisterMessage(LMScoreResponse)


import grpc
from grpc.beta import implementations as beta_implementations
from grpc.beta import interfaces as beta_interfaces
from grpc.framework.common import cardinality
from grpc.framework.interfaces.face import utilities as face_utilities


class LanguageModelStub(object):
  """This is the service for our API
  """

  def __init__(self, channel):
    """Constructor.

    Args:
      channel: A grpc.Channel.
    """
    self.Score = channel.unary_unary(
        '/lm.LanguageModel/Score',
        request_serializer=SentenceRequest.SerializeToString,
        response_deserializer=LMScoreResponse.FromString,
        )


class LanguageModelServicer(object):
  """This is the service for our API
  """

  def Score(self, request, context):
    context.set_code(grpc.StatusCode.UNIMPLEMENTED)
    context.set_details('Method not implemented!')
    raise NotImplementedError('Method not implemented!')


def add_LanguageModelServicer_to_server(servicer, server):
  rpc_method_handlers = {
      'Score': grpc.unary_unary_rpc_method_handler(
          servicer.Score,
          request_deserializer=SentenceRequest.FromString,
          response_serializer=LMScoreResponse.SerializeToString,
      ),
  }
  generic_handler = grpc.method_handlers_generic_handler(
      'lm.LanguageModel', rpc_method_handlers)
  server.add_generic_rpc_handlers((generic_handler,))


class BetaLanguageModelServicer(object):
  """The Beta API is deprecated for 0.15.0 and later.

  It is recommended to use the GA API (classes and functions in this
  file not marked beta) for all further purposes. This class was generated
  only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0."""
  """This is the service for our API
  """
  def Score(self, request, context):
    context.code(beta_interfaces.StatusCode.UNIMPLEMENTED)


class BetaLanguageModelStub(object):
  """The Beta API is deprecated for 0.15.0 and later.

  It is recommended to use the GA API (classes and functions in this
  file not marked beta) for all further purposes. This class was generated
  only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0."""
  """This is the service for our API
  """
  def Score(self, request, timeout, metadata=None, with_call=False, protocol_options=None):
    raise NotImplementedError()
  Score.future = None


def beta_create_LanguageModel_server(servicer, pool=None, pool_size=None, default_timeout=None, maximum_timeout=None):
  """The Beta API is deprecated for 0.15.0 and later.

  It is recommended to use the GA API (classes and functions in this
  file not marked beta) for all further purposes. This function was
  generated only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0"""
  request_deserializers = {
    ('lm.LanguageModel', 'Score'): SentenceRequest.FromString,
  }
  response_serializers = {
    ('lm.LanguageModel', 'Score'): LMScoreResponse.SerializeToString,
  }
  method_implementations = {
    ('lm.LanguageModel', 'Score'): face_utilities.unary_unary_inline(servicer.Score),
  }
  server_options = beta_implementations.server_options(request_deserializers=request_deserializers, response_serializers=response_serializers, thread_pool=pool, thread_pool_size=pool_size, default_timeout=default_timeout, maximum_timeout=maximum_timeout)
  return beta_implementations.server(method_implementations, options=server_options)


def beta_create_LanguageModel_stub(channel, host=None, metadata_transformer=None, pool=None, pool_size=None):
  """The Beta API is deprecated for 0.15.0 and later.

  It is recommended to use the GA API (classes and functions in this
  file not marked beta) for all further purposes. This function was
  generated only to ease transition from grpcio<0.15.0 to grpcio>=0.15.0"""
  request_serializers = {
    ('lm.LanguageModel', 'Score'): SentenceRequest.SerializeToString,
  }
  response_deserializers = {
    ('lm.LanguageModel', 'Score'): LMScoreResponse.FromString,
  }
  cardinalities = {
    'Score': cardinality.Cardinality.UNARY_UNARY,
  }
  stub_options = beta_implementations.stub_options(host=host, metadata_transformer=metadata_transformer, request_serializers=request_serializers, response_deserializers=response_deserializers, thread_pool=pool, thread_pool_size=pool_size)
  return beta_implementations.dynamic_stub(channel, 'lm.LanguageModel', cardinalities, options=stub_options)
# @@protoc_insertion_point(module_scope)
