sparklm{
  inputs = ["hdfs://hadoop1.dialoid.com//home/lm/data/search/important_sent.txt"]
//            "hdfs://hadoop1.dialoid.com//home/lm/data/search/yearly/*.txt"]
  output_dir  = "hdfs://hadoop1.dialoid.com/home/lm/out/arpa2"
  output_format = "arpa"
  debug = true
  
  dictionary {
    is_provided = false

    start_symbol = "<s>"
    end_symbol = "</s>"
    unk = "<UNK>"

    word_count = 1000000
    save_path = ""
  
    file_name = "wordDict.txt"
  }

  ngram {
    order = 3
    discount = 0.5
    prune_cnt = 1
    max_mod_kn_special_count = 3
    min_counts_pre = [1,1,2,2,2,2,2,2,2,2,2]
    ignore_qc = true
  }

  tokenizer = "twitter"
    
}
